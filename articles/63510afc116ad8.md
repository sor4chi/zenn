---
title: "RustでJSONの構文解析がしたい"
emoji: "📚"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["rust", "parser", "json"]
published: true
---

:::message
Rustを始めて書くのであまり鵜呑みにしないようにお願いします。
構文解析の知識、アルゴリズムに関しては間違っている可能性があります、もしあれば教えていただけると嬉しいです。
:::

# 経緯

プログラミング言語論の講義を大学で履修しており、構文解析が面白かったのでRustで実装してみたいと思いました。なるべく学んだことを対応づけながら実装していきます。

ちなみに作ったものを学友に教えるのが目的なので、あまり高度なものは作りたくない...。
-> **Json Parserだったら楽では**
ってことでJsonの構文解析をやってみます。

脳死でやりたいのでなんちゃってTDDでやります
単にParserの実装をしても面白くないので、Parser -> Formatter -> Cliとして派生させていきます。

**成果物**
https://github.com/sor4chi/json-parser

# 字句解析

まず最初に字句解析をします。

まずはJSONの構成に必要なTokenを定義します。
https://ja.wikipedia.org/wiki/%E5%AD%97%E5%8F%A5%E8%A7%A3%E6%9E%90
> 計算機科学における字句解析 (じくかいせき、英: lexical analysis) とは、広義の構文解析の前半の処理で、自然言語の文やプログラミング言語のソースコードなどの文字列を解析して、後半の狭義の構文解析で最小単位（終端記号）となっている「トークン」（字句）の並びを得る手続きである。字句解析を行うプログラムは字句解析器である。

作ってからWikiを確認しましたがやっぱりTokenっていわゆる終端記号のことだったっぽい？

:::details 考察 *終端記号とToken*
言語の理論でチョムスキー標準形など、言語の生成規則の標準化をするプロセス内で終端を明示的にする表現が出てきます。

$S \rightarrow AB | ASB$
$A \rightarrow a$
$B \rightarrow b$

このように終端記号を弾き出す$a$や$b$のことを終端記号、いわゆるTokenと呼ぶのかな？
こういう標準化のやりたいことがなんとなく理解できる。形式言語から構文を推測するための手法？
:::

実装し始めてすぐ気づいて直したのですが、NumberやStringといったリテラルのものをTokenとして扱うと定義や処理が複雑になってしまいます。これは上記Wikiにも全く同じことが書いてあって安心しました。
Token = 終端記号とはせず、リテラルも一塊のTokenとして扱うようにした方が処理上都合いいとわかりました。

:::details 考察 *JSONのBNFと終端記号の話*
リポジトリにJSONをBNF記法で定義したもの。
https://github.com/sor4chi/json-parser/blob/main/json.bnf
[BNF Playground](https://bnfplayground.pauliankline.com/?bnf=%3Cjson%3E%20%3A%3A%3D%20%3Cobject%3E%20%7C%20%3Carray%3E%0A%3Cobject%3E%20%3A%3A%3D%20%22%7B%22%20%3Cmembers%3E%20%22%7D%22%0A%3Cmembers%3E%20%3A%3A%3D%20%3Cpair%3E%20%7C%20%3Cpair%3E%20%22%2C%22%20%3Cmembers%3E%0A%3Cpair%3E%20%3A%3A%3D%20%3Cstring%3E%20%22%3A%22%20%3Cvalue%3E%0A%3Carray%3E%20%3A%3A%3D%20%22%5B%22%20%3Celements%3E%20%22%5D%22%0A%3Celements%3E%20%3A%3A%3D%20%3Cvalue%3E%20%7C%20%3Cvalue%3E%20%22%2C%22%20%3Celements%3E%0A%3Cvalue%3E%20%3A%3A%3D%20%3Cstring%3E%20%7C%20%3Cnumber%3E%20%7C%20%3Cobject%3E%20%7C%20%3Carray%3E%20%7C%20%22true%22%20%7C%20%22false%22%20%7C%20%22null%22%0A%3Cstring%3E%20%3A%3A%3D%20%22%27%22%20%3Ccharacters%3E%20%22%27%22%0A%3Ccharacters%3E%20%3A%3A%3D%20%3Ccharacter%3E%20%7C%20%3Ccharacter%3E%20%3Ccharacters%3E%0A%3Ccharacter%3E%20%3A%3A%3D%20%5Ba-z%5D%0A%3Cnumber%3E%20%3A%3A%3D%20%3Cinteger%3E%20%7C%20%3Cinteger%3E%20%22.%22%20%3Cfraction%3E%20%7C%20%3Cinteger%3E%20%22.%22%20%3Cfraction%3E%20%3Cexponent%3E%20%7C%20%3Cinteger%3E%20%3Cexponent%3E%0A%3Cinteger%3E%20%3A%3A%3D%20%3Cdigit%3E%20%7C%20%3Cdigit%3E%20%3Cinteger%3E%0A%3Cdigit%3E%20%3A%3A%3D%20%5B0-9%5D%0A%3Cfraction%3E%20%3A%3A%3D%20%3Cdigit%3E%20%3Cfraction%3E%0A%3Cexponent%3E%20%3A%3A%3D%20%3Cexponent%3E%20%3Cexponent%3E%0A&name=)
を書いて置いておきましたが、このダブルクオートで囲われた記号にあたるのがTokenというわけですね？
:::

なので、Tokenは以下のように定義します。

```rust
#[derive(Debug, PartialEq, PartialOrd, Clone)]
pub enum Token {
    LBrace,
    RBrace,
    LBracket,
    RBracket,
    Colon,
    Comma,
    StringValue(String),
    NumberValue(f64),
    BooleanValue(bool),
    NullValue,
    End,
}
```

Rustのenumは構造体のように扱えるので、StringやNumberなどのリテラルをTokenに含めることができます。この性質を利用して先ほどの拡張された定義でのTokenを実装します。

## テスト

TDDなんで先にテストを書きます。

```rust
#[test]
fn test_tokenize() {
    let input = r#"{"foo":123}"#;
    let mut lexer = Lexer::new(input);
    let tests: Vec<Token> = vec![
        Token::LBrace,                         // {
        Token::StringValue("foo".to_string()), // "foo"
        Token::Colon,                          // :
        Token::NumberValue(123.0),             // 123
        Token::RBrace,                         // }
        Token::End,                            // end
    ];

    for test in tests {
        assert_eq!(lexer.next_token(), test);
    }
}
```

これが通れば一応簡易的なLexerは完成です。

## 実装

Lexerを実装します。（コードは全文載せると長いので一部抜粋しています）

まず、Lexerを作るにあたってPeekという構造を使います。
Peekは入力を1Itemずつ読み込むいわばストリームで、カーソル（ポインタ）を進めずに次のItemを覗き見る（peekする）ことができます。
そして次のItemにカーソルを進めるとそれはそのItemを消費した（consumeした）ことになり、そのItemは破棄されます。

字句解析では、Inputが`Vec<char>`で現在見ている文字と次の文字を見る必要があるので、Peekというデータ構造がピッタリのようです。

まずVecをPeekableなIteratorに変換するユーティリティ型を定義します。これは後の構文解析でも使うので、`src/utility.rs`に定義しました。

```rust
use std::iter::Peekable;
use std::vec::IntoIter;

pub type PeekableIter<T> = Peekable<IntoIter<T>>;
```

次にLexerを定義します。LexerはPeekableなIterator`char_stream`を持ちます。
あとはnewメソッドでLexerを作成できるようにします。

```rust
pub struct Lexer {
    char_stream: PeekableIter<char>,
}

impl Lexer {
    pub fn new(input: &str) -> Self {
        Lexer {
            char_stream: input.chars().collect::<Vec<char>>().into_iter().peekable(),
        }
    }
}
```

次に、LexerのTokenをPhfMapで定義します。これは`src/token.rs`に定義しました。
PhfMapはRustのマクロで、コンパイル時にハッシュマップを作成します。静的なハッシュマップを作成するので、実行時にはハッシュマップを作成する必要がなく、高速に動作して便利です。

```rust
pub static CHAR_TOKENS: phf::Map<char, Token> = phf_map! {
    '{' => Token::LBrace,
    '}' => Token::RBrace,
    '[' => Token::LBracket,
    ']' => Token::RBracket,
    ':' => Token::Colon,
    ',' => Token::Comma,
};

pub static KEYWORD_TOKENS: phf::Map<&'static str, Token> = phf_map! {
    "true" => Token::BooleanValue(true),
    "false" => Token::BooleanValue(false),
    "null" => Token::NullValue,
};
```

ここでは、`{`や`}`などのCharをTokenに変換するためのマップと、`true`や`false`などのKeywordをTokenに変換するためのマップを定義しています。

次に、Lexerのconsume_charメソッドを実装します。これはPeekableなIteratorのnextメソッドを呼び出して、次の文字を消費します。

### 文字消費

#### テスト

まずはテストを書きます。

```rust
#[test]
fn test_consume_char() {
    let input = r#"{}[]:,"#;
    let mut lexer = Lexer::new(input);
    assert_eq!(lexer.consume_char(), Token::LBrace); // {
    assert_eq!(lexer.consume_char(), Token::RBrace); // }
    assert_eq!(lexer.consume_char(), Token::LBracket); // [
    assert_eq!(lexer.consume_char(), Token::RBracket); // ]
    assert_eq!(lexer.consume_char(), Token::Colon); // :
    assert_eq!(lexer.consume_char(), Token::Comma); // ,
}
```

これが通ればconsume_charメソッドは完成です。

#### 実装

```rust
impl Lexer {
    fn consume_char(&mut self) -> Token {
        match self.char_stream.next() {
            Some(c) => match CHAR_TOKENS.get(&c) {
                Some(token) => token.clone(),
                None => Token::Unknown,
            },
            None => Token::End,
        }
    }
}
```

これで、Lexerのconsume_charメソッドは完成です。

### 数値消費

#### テスト

次に数値を消費するテストを書きます。

```rust
#[test]
fn test_consume_number() {
    let input = r#"123"#;
    let mut lexer = Lexer::new(input);
    assert_eq!(lexer.consume_number(), Token::NumberValue(123.0));
}
```

これが通ればconsume_numberメソッドは完成です。

#### 実装

```rust
impl Lexer {
    fn consume_number(&mut self) -> Token {
        let mut s = String::new();
        loop {
            match self.char_stream.peek() {
                Some(c) if c.is_numeric() || c == &'.' => match self.char_stream.next() {
                    Some(c) => s.push(c),
                    None => panic!("Unexpected end of number"),
                },
                _ => break,
            }
        }
        match s.parse::<f64>() {
            Ok(n) => Token::NumberValue(n),
            Err(_) => panic!("Unexpected number: {}", s),
        }
    }
}
```

これで、Lexerのconsume_numberメソッドは完成です。
