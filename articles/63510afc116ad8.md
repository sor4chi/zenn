---
title: "Rustã§JSONã®æ§‹æ–‡è§£æãŒã—ãŸã„"
emoji: "ğŸ“š"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["rust", "parser", "json"]
published: true
---

:::message
Rustã‚’å§‹ã‚ã¦æ›¸ãã®ã§ã‚ã¾ã‚Šéµœå‘‘ã¿ã«ã—ãªã„ã‚ˆã†ã«ãŠé¡˜ã„ã—ã¾ã™ã€‚
æ§‹æ–‡è§£æã®çŸ¥è­˜ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é–¢ã—ã¦ã¯é–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€ã‚‚ã—ã‚ã‚Œã°æ•™ãˆã¦ã„ãŸã ã‘ã‚‹ã¨å¬‰ã—ã„ã§ã™ã€‚
:::

# çµŒç·¯

ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªè«–ã®è¬›ç¾©ã‚’å¤§å­¦ã§å±¥ä¿®ã—ã¦ãŠã‚Šã€æ§‹æ–‡è§£æãŒé¢ç™½ã‹ã£ãŸã®ã§Rustã§å®Ÿè£…ã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã—ãŸã€‚ãªã‚‹ã¹ãå­¦ã‚“ã ã“ã¨ã‚’å¯¾å¿œã¥ã‘ãªãŒã‚‰å®Ÿè£…ã—ã¦ã„ãã¾ã™ã€‚

ã¡ãªã¿ã«ä½œã£ãŸã‚‚ã®ã‚’å­¦å‹ã«æ•™ãˆã‚‹ã®ãŒç›®çš„ãªã®ã§ã€ã‚ã¾ã‚Šé«˜åº¦ãªã‚‚ã®ã¯ä½œã‚ŠãŸããªã„...ã€‚
-> **Json Parserã ã£ãŸã‚‰æ¥½ã§ã¯**
ã£ã¦ã“ã¨ã§Jsonã®æ§‹æ–‡è§£æã‚’ã‚„ã£ã¦ã¿ã¾ã™ã€‚

è„³æ­»ã§ã‚„ã‚ŠãŸã„ã®ã§ãªã‚“ã¡ã‚ƒã£ã¦TDDã§ã‚„ã‚Šã¾ã™
å˜ã«Parserã®å®Ÿè£…ã‚’ã—ã¦ã‚‚é¢ç™½ããªã„ã®ã§ã€Parser -> Formatter -> Cliã¨ã—ã¦æ´¾ç”Ÿã•ã›ã¦ã„ãã¾ã™ã€‚

**æˆæœç‰©**
https://github.com/sor4chi/json-parser

# å­—å¥è§£æ

ã¾ãšæœ€åˆã«å­—å¥è§£æã‚’ã—ã¾ã™ã€‚

ã¾ãšã¯JSONã®æ§‹æˆã«å¿…è¦ãªTokenã‚’å®šç¾©ã—ã¾ã™ã€‚
https://ja.wikipedia.org/wiki/%E5%AD%97%E5%8F%A5%E8%A7%A3%E6%9E%90
> è¨ˆç®—æ©Ÿç§‘å­¦ã«ãŠã‘ã‚‹å­—å¥è§£æ (ã˜ãã‹ã„ã›ãã€è‹±: lexical analysis) ã¨ã¯ã€åºƒç¾©ã®æ§‹æ–‡è§£æã®å‰åŠã®å‡¦ç†ã§ã€è‡ªç„¶è¨€èªã®æ–‡ã‚„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ãªã©ã®æ–‡å­—åˆ—ã‚’è§£æã—ã¦ã€å¾ŒåŠã®ç‹­ç¾©ã®æ§‹æ–‡è§£æã§æœ€å°å˜ä½ï¼ˆçµ‚ç«¯è¨˜å·ï¼‰ã¨ãªã£ã¦ã„ã‚‹ã€Œãƒˆãƒ¼ã‚¯ãƒ³ã€ï¼ˆå­—å¥ï¼‰ã®ä¸¦ã³ã‚’å¾—ã‚‹æ‰‹ç¶šãã§ã‚ã‚‹ã€‚å­—å¥è§£æã‚’è¡Œã†ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯å­—å¥è§£æå™¨ã§ã‚ã‚‹ã€‚

ä½œã£ã¦ã‹ã‚‰Wikiã‚’ç¢ºèªã—ã¾ã—ãŸãŒã‚„ã£ã±ã‚ŠTokenã£ã¦ã„ã‚ã‚†ã‚‹çµ‚ç«¯è¨˜å·ã®ã“ã¨ã ã£ãŸã£ã½ã„ï¼Ÿ

:::details è€ƒå¯Ÿ *çµ‚ç«¯è¨˜å·ã¨Token*
è¨€èªã®ç†è«–ã§ãƒãƒ§ãƒ ã‚¹ã‚­ãƒ¼æ¨™æº–å½¢ãªã©ã€è¨€èªã®ç”Ÿæˆè¦å‰‡ã®æ¨™æº–åŒ–ã‚’ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹å†…ã§çµ‚ç«¯ã‚’æ˜ç¤ºçš„ã«ã™ã‚‹è¡¨ç¾ãŒå‡ºã¦ãã¾ã™ã€‚

$S \rightarrow AB | ASB$
$A \rightarrow a$
$B \rightarrow b$

ã“ã®ã‚ˆã†ã«çµ‚ç«¯è¨˜å·ã‚’å¼¾ãå‡ºã™$a$ã‚„$b$ã®ã“ã¨ã‚’çµ‚ç«¯è¨˜å·ã€ã„ã‚ã‚†ã‚‹Tokenã¨å‘¼ã¶ã®ã‹ãªï¼Ÿ
ã“ã†ã„ã†æ¨™æº–åŒ–ã®ã‚„ã‚ŠãŸã„ã“ã¨ãŒãªã‚“ã¨ãªãç†è§£ã§ãã‚‹ã€‚å½¢å¼è¨€èªã‹ã‚‰æ§‹æ–‡ã‚’æ¨æ¸¬ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ï¼Ÿ
:::

å®Ÿè£…ã—å§‹ã‚ã¦ã™ãæ°—ã¥ã„ã¦ç›´ã—ãŸã®ã§ã™ãŒã€Numberã‚„Stringã¨ã„ã£ãŸãƒªãƒ†ãƒ©ãƒ«ã®ã‚‚ã®ã‚’Tokenã¨ã—ã¦æ‰±ã†ã¨å®šç¾©ã‚„å‡¦ç†ãŒè¤‡é›‘ã«ãªã£ã¦ã—ã¾ã„ã¾ã™ã€‚ã“ã‚Œã¯ä¸Šè¨˜Wikiã«ã‚‚å…¨ãåŒã˜ã“ã¨ãŒæ›¸ã„ã¦ã‚ã£ã¦å®‰å¿ƒã—ã¾ã—ãŸã€‚
Token = çµ‚ç«¯è¨˜å·ã¨ã¯ã›ãšã€ãƒªãƒ†ãƒ©ãƒ«ã‚‚ä¸€å¡Šã®Tokenã¨ã—ã¦æ‰±ã†ã‚ˆã†ã«ã—ãŸæ–¹ãŒå‡¦ç†ä¸Šéƒ½åˆã„ã„ã¨ã‚ã‹ã‚Šã¾ã—ãŸã€‚

:::details è€ƒå¯Ÿ *JSONã®BNFã¨çµ‚ç«¯è¨˜å·ã®è©±*
ãƒªãƒã‚¸ãƒˆãƒªã«JSONã‚’BNFè¨˜æ³•ã§å®šç¾©ã—ãŸã‚‚ã®ã€‚
https://github.com/sor4chi/json-parser/blob/main/json.bnf
[BNF Playground](https://bnfplayground.pauliankline.com/?bnf=%3Cjson%3E%20%3A%3A%3D%20%3Cobject%3E%20%7C%20%3Carray%3E%0A%3Cobject%3E%20%3A%3A%3D%20%22%7B%22%20%3Cmembers%3E%20%22%7D%22%0A%3Cmembers%3E%20%3A%3A%3D%20%3Cpair%3E%20%7C%20%3Cpair%3E%20%22%2C%22%20%3Cmembers%3E%0A%3Cpair%3E%20%3A%3A%3D%20%3Cstring%3E%20%22%3A%22%20%3Cvalue%3E%0A%3Carray%3E%20%3A%3A%3D%20%22%5B%22%20%3Celements%3E%20%22%5D%22%0A%3Celements%3E%20%3A%3A%3D%20%3Cvalue%3E%20%7C%20%3Cvalue%3E%20%22%2C%22%20%3Celements%3E%0A%3Cvalue%3E%20%3A%3A%3D%20%3Cstring%3E%20%7C%20%3Cnumber%3E%20%7C%20%3Cobject%3E%20%7C%20%3Carray%3E%20%7C%20%22true%22%20%7C%20%22false%22%20%7C%20%22null%22%0A%3Cstring%3E%20%3A%3A%3D%20%22%27%22%20%3Ccharacters%3E%20%22%27%22%0A%3Ccharacters%3E%20%3A%3A%3D%20%3Ccharacter%3E%20%7C%20%3Ccharacter%3E%20%3Ccharacters%3E%0A%3Ccharacter%3E%20%3A%3A%3D%20%5Ba-z%5D%0A%3Cnumber%3E%20%3A%3A%3D%20%3Cinteger%3E%20%7C%20%3Cinteger%3E%20%22.%22%20%3Cfraction%3E%20%7C%20%3Cinteger%3E%20%22.%22%20%3Cfraction%3E%20%3Cexponent%3E%20%7C%20%3Cinteger%3E%20%3Cexponent%3E%0A%3Cinteger%3E%20%3A%3A%3D%20%3Cdigit%3E%20%7C%20%3Cdigit%3E%20%3Cinteger%3E%0A%3Cdigit%3E%20%3A%3A%3D%20%5B0-9%5D%0A%3Cfraction%3E%20%3A%3A%3D%20%3Cdigit%3E%20%3Cfraction%3E%0A%3Cexponent%3E%20%3A%3A%3D%20%3Cexponent%3E%20%3Cexponent%3E%0A&name=)
ã‚’æ›¸ã„ã¦ç½®ã„ã¦ãŠãã¾ã—ãŸãŒã€ã“ã®ãƒ€ãƒ–ãƒ«ã‚¯ã‚ªãƒ¼ãƒˆã§å›²ã‚ã‚ŒãŸè¨˜å·ã«ã‚ãŸã‚‹ã®ãŒTokenã¨ã„ã†ã‚ã‘ã§ã™ã­ï¼Ÿ
:::

ãªã®ã§ã€Tokenã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™ã€‚

```rust
#[derive(Debug, PartialEq, PartialOrd, Clone)]
pub enum Token {
    LBrace,
    RBrace,
    LBracket,
    RBracket,
    Colon,
    Comma,
    StringValue(String),
    NumberValue(f64),
    BooleanValue(bool),
    NullValue,
    End,
}
```

Rustã®enumã¯æ§‹é€ ä½“ã®ã‚ˆã†ã«æ‰±ãˆã‚‹ã®ã§ã€Stringã‚„Numberãªã©ã®ãƒªãƒ†ãƒ©ãƒ«ã‚’Tokenã«å«ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®æ€§è³ªã‚’åˆ©ç”¨ã—ã¦å…ˆã»ã©ã®æ‹¡å¼µã•ã‚ŒãŸå®šç¾©ã§ã®Tokenã‚’å®Ÿè£…ã—ã¾ã™ã€‚

## ãƒ†ã‚¹ãƒˆ

TDDãªã‚“ã§å…ˆã«ãƒ†ã‚¹ãƒˆã‚’æ›¸ãã¾ã™ã€‚

```rust
#[test]
fn test_tokenize() {
    let input = r#"{"foo":123}"#;
    let mut lexer = Lexer::new(input);
    let tests: Vec<Token> = vec![
        Token::LBrace,                         // {
        Token::StringValue("foo".to_string()), // "foo"
        Token::Colon,                          // :
        Token::NumberValue(123.0),             // 123
        Token::RBrace,                         // }
        Token::End,                            // end
    ];

    for test in tests {
        assert_eq!(lexer.next_token(), test);
    }
}
```

ã“ã‚ŒãŒé€šã‚Œã°ä¸€å¿œç°¡æ˜“çš„ãªLexerã¯å®Œæˆã§ã™ã€‚

## å®Ÿè£…

Lexerã‚’å®Ÿè£…ã—ã¾ã™ã€‚ï¼ˆã‚³ãƒ¼ãƒ‰ã¯å…¨æ–‡è¼‰ã›ã‚‹ã¨é•·ã„ã®ã§ä¸€éƒ¨æŠœç²‹ã—ã¦ã„ã¾ã™ï¼‰

ã¾ãšã€Lexerã‚’ä½œã‚‹ã«ã‚ãŸã£ã¦Peekã¨ã„ã†æ§‹é€ ã‚’ä½¿ã„ã¾ã™ã€‚
Peekã¯å…¥åŠ›ã‚’1Itemãšã¤èª­ã¿è¾¼ã‚€ã„ã‚ã°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§ã€ã‚«ãƒ¼ã‚½ãƒ«ï¼ˆãƒã‚¤ãƒ³ã‚¿ï¼‰ã‚’é€²ã‚ãšã«æ¬¡ã®Itemã‚’è¦—ãè¦‹ã‚‹ï¼ˆpeekã™ã‚‹ï¼‰ã“ã¨ãŒã§ãã¾ã™ã€‚
ãã—ã¦æ¬¡ã®Itemã«ã‚«ãƒ¼ã‚½ãƒ«ã‚’é€²ã‚ã‚‹ã¨ãã‚Œã¯ãã®Itemã‚’æ¶ˆè²»ã—ãŸï¼ˆconsumeã—ãŸï¼‰ã“ã¨ã«ãªã‚Šã€ãã®Itemã¯ç ´æ£„ã•ã‚Œã¾ã™ã€‚

å­—å¥è§£æã§ã¯ã€InputãŒ`Vec<char>`ã§ç¾åœ¨è¦‹ã¦ã„ã‚‹æ–‡å­—ã¨æ¬¡ã®æ–‡å­—ã‚’è¦‹ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã€Peekã¨ã„ã†ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒãƒ”ãƒƒã‚¿ãƒªã®ã‚ˆã†ã§ã™ã€‚

ã¾ãšVecã‚’PeekableãªIteratorã«å¤‰æ›ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£å‹ã‚’å®šç¾©ã—ã¾ã™ã€‚ã“ã‚Œã¯å¾Œã®æ§‹æ–‡è§£æã§ã‚‚ä½¿ã†ã®ã§ã€`src/utility.rs`ã«å®šç¾©ã—ã¾ã—ãŸã€‚

```rust
use std::iter::Peekable;
use std::vec::IntoIter;

pub type PeekableIter<T> = Peekable<IntoIter<T>>;
```

æ¬¡ã«Lexerã‚’å®šç¾©ã—ã¾ã™ã€‚Lexerã¯PeekableãªIterator`char_stream`ã‚’æŒã¡ã¾ã™ã€‚
ã‚ã¨ã¯newãƒ¡ã‚½ãƒƒãƒ‰ã§Lexerã‚’ä½œæˆã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```rust
pub struct Lexer {
    char_stream: PeekableIter<char>,
}

impl Lexer {
    pub fn new(input: &str) -> Self {
        Lexer {
            char_stream: input.chars().collect::<Vec<char>>().into_iter().peekable(),
        }
    }
}
```

æ¬¡ã«ã€Lexerã®Tokenã‚’PhfMapã§å®šç¾©ã—ã¾ã™ã€‚ã“ã‚Œã¯`src/token.rs`ã«å®šç¾©ã—ã¾ã—ãŸã€‚
PhfMapã¯Rustã®ãƒã‚¯ãƒ­ã§ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«ãƒãƒƒã‚·ãƒ¥ãƒãƒƒãƒ—ã‚’ä½œæˆã—ã¾ã™ã€‚é™çš„ãªãƒãƒƒã‚·ãƒ¥ãƒãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹ã®ã§ã€å®Ÿè¡Œæ™‚ã«ã¯ãƒãƒƒã‚·ãƒ¥ãƒãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒãªãã€é«˜é€Ÿã«å‹•ä½œã—ã¦ä¾¿åˆ©ã§ã™ã€‚

```rust
pub static CHAR_TOKENS: phf::Map<char, Token> = phf_map! {
    '{' => Token::LBrace,
    '}' => Token::RBrace,
    '[' => Token::LBracket,
    ']' => Token::RBracket,
    ':' => Token::Colon,
    ',' => Token::Comma,
};

pub static KEYWORD_TOKENS: phf::Map<&'static str, Token> = phf_map! {
    "true" => Token::BooleanValue(true),
    "false" => Token::BooleanValue(false),
    "null" => Token::NullValue,
};
```

ã“ã“ã§ã¯ã€`{`ã‚„`}`ãªã©ã®Charã‚’Tokenã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ãƒãƒƒãƒ—ã¨ã€`true`ã‚„`false`ãªã©ã®Keywordã‚’Tokenã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ãƒãƒƒãƒ—ã‚’å®šç¾©ã—ã¦ã„ã¾ã™ã€‚

æ¬¡ã«ã€Lexerã®consume_charãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ã“ã‚Œã¯PeekableãªIteratorã®nextãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—ã¦ã€æ¬¡ã®æ–‡å­—ã‚’æ¶ˆè²»ã—ã¾ã™ã€‚

### æ–‡å­—æ¶ˆè²»

#### ãƒ†ã‚¹ãƒˆ

ã¾ãšã¯ãƒ†ã‚¹ãƒˆã‚’æ›¸ãã¾ã™ã€‚

```rust
#[test]
fn test_consume_char() {
    let input = r#"{}[]:,"#;
    let mut lexer = Lexer::new(input);
    assert_eq!(lexer.consume_char(), Token::LBrace); // {
    assert_eq!(lexer.consume_char(), Token::RBrace); // }
    assert_eq!(lexer.consume_char(), Token::LBracket); // [
    assert_eq!(lexer.consume_char(), Token::RBracket); // ]
    assert_eq!(lexer.consume_char(), Token::Colon); // :
    assert_eq!(lexer.consume_char(), Token::Comma); // ,
}
```

ã“ã‚ŒãŒé€šã‚Œã°consume_charãƒ¡ã‚½ãƒƒãƒ‰ã¯å®Œæˆã§ã™ã€‚

#### å®Ÿè£…

```rust
impl Lexer {
    fn consume_char(&mut self) -> Token {
        match self.char_stream.next() {
            Some(c) => match CHAR_TOKENS.get(&c) {
                Some(token) => token.clone(),
                None => Token::Unknown,
            },
            None => Token::End,
        }
    }
}
```

ã“ã‚Œã§ã€Lexerã®consume_charãƒ¡ã‚½ãƒƒãƒ‰ã¯å®Œæˆã§ã™ã€‚

### æ•°å€¤æ¶ˆè²»

#### ãƒ†ã‚¹ãƒˆ

æ¬¡ã«æ•°å€¤ã‚’æ¶ˆè²»ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚’æ›¸ãã¾ã™ã€‚

```rust
#[test]
fn test_consume_number() {
    let input = r#"123"#;
    let mut lexer = Lexer::new(input);
    assert_eq!(lexer.consume_number(), Token::NumberValue(123.0));
}
```

ã“ã‚ŒãŒé€šã‚Œã°consume_numberãƒ¡ã‚½ãƒƒãƒ‰ã¯å®Œæˆã§ã™ã€‚

#### å®Ÿè£…

```rust
impl Lexer {
    fn consume_number(&mut self) -> Token {
        let mut s = String::new();
        loop {
            match self.char_stream.peek() {
                Some(c) if c.is_numeric() || c == &'.' => match self.char_stream.next() {
                    Some(c) => s.push(c),
                    None => panic!("Unexpected end of number"),
                },
                _ => break,
            }
        }
        match s.parse::<f64>() {
            Ok(n) => Token::NumberValue(n),
            Err(_) => panic!("Unexpected number: {}", s),
        }
    }
}
```

ã“ã‚Œã§ã€Lexerã®consume_numberãƒ¡ã‚½ãƒƒãƒ‰ã¯å®Œæˆã§ã™ã€‚
